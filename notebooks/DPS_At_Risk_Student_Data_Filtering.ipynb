{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v62RaemU3YPq"
   },
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-PLcDGG3qKU",
    "outputId": "987b6381-07c3-41a9-95ff-d57a16455e2b"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define URL and Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://webfs.oecd.org/pisa2022/STU_QQQ_SAS.zip\"\n",
    "zip_filename = \"SAS_STU_QQQ.zip\"\n",
    "target_extracted_file = \"CY08MSP_STU_QQQ.sas7bdat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the File (With Stream Enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mr3KraCC31Ud",
    "outputId": "c464507a-43ba-4a9e-d1a1-96842acb2276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100.0% (737 MB)\n",
      "Download complete! Saved as SAS_STU_QQQ.zip\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status() # Check for download errors\n",
    "\n",
    "    # Download in chunks to save memory and show progress\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 * 1024 # 1MB chunks\n",
    "    wrote = 0\n",
    "\n",
    "    with open(zip_filename, 'wb') as f:\n",
    "        for data in response.iter_content(block_size):\n",
    "            wrote = wrote + len(data)\n",
    "            f.write(data)\n",
    "            # Simple text progress bar\n",
    "            if total_size > 0:\n",
    "                percent = wrote / total_size * 100\n",
    "                print(f\"\\rDownloading: {percent:.1f}% ({wrote//(1024*1024)} MB)\", end=\"\")\n",
    "    \n",
    "    print(f\"\\nDownload complete! Saved as {zip_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract The File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CY08MSP_STU_QQQ.sas7bdat...\n",
      "['CY08MSP_STU_QQQ.FORMAT.SAS', 'CY08MSP_STU_QQQ.SAS7BDAT']\n",
      "Success! Extracted: CY08MSP_STU_QQQ.SAS7BDAT\n",
      "Renamed to: CY08MSP_STU_QQQ.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "print(f\"Extracting {target_extracted_file}...\")\n",
    "with zipfile.ZipFile(zip_filename, 'r') as z:\n",
    "    # Check if the file exists in the zip\n",
    "    # Note: Sometimes the internal name varies slightly, so we list files first to be safe\n",
    "    file_list = z.namelist()\n",
    "    print(file_list)\n",
    "    # Look for the .sas7bdat file\n",
    "    sas_file = [f for f in file_list if f.endswith('.SAS7BDAT')][0]\n",
    "    \n",
    "    z.extract(sas_file, \".\")\n",
    "    print(f\"Success! Extracted: {sas_file}\")\n",
    "    \n",
    "    # Optional: Rename to the standard name if it differs\n",
    "    if sas_file != target_extracted_file:\n",
    "        os.rename(sas_file, target_extracted_file)\n",
    "        print(f\"Renamed to: {target_extracted_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx0e1_5W6du1"
   },
   "source": [
    "## Get the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1_Cbl6l6m9Z",
    "outputId": "b418f5c7-51a0-4fc8-e06e-d495dc479bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyreadstat in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: narwhals>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyreadstat) (2.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyreadstat) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1hZHBoL76r5S"
   },
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FohqK-_767U6"
   },
   "source": [
    "## Filter data for Philippines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZzLjCiMq6vpC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered for Philippines. Student count: 7193\n"
     ]
    }
   ],
   "source": [
    "df, meta = pyreadstat.read_sas7bdat(target_extracted_file)\n",
    "\n",
    "ph_df = df[df['CNT'] == 'PHL'].copy()\n",
    "print(f\"Filtered for Philippines. Student count: {len(ph_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill ANXMATH\n",
    "Since we don't have ANXMATH for PH, what we'll do is to try to re-create the values, by doing the following:\n",
    "\n",
    "1. Search by \"Question Text\": Variable names (like ST246) change, but the question text (e.g., \"I get very tense\") stays the same. We will search the metadata labels.\n",
    "2. Use a Proxy (Self-Efficacy): If Anxiety is truly missing, Math Self-Efficacy (MATHEFF) is scientifically proven to be the inverse of Anxiety (Low Efficacy $\\approx$ High Anxiety).\n",
    "3. Check for \"General Anxiety\": Sometimes it's labeled under general well-being (STRRESS, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching metadata for 'Anxiety', 'Worry', 'Tense', or 'Math'...\n",
      "\n",
      "Found 13 potential Anxiety/Emotion variables:\n",
      " - ST322Q07JA: How often:  I feel nervous/anxious when I don't have my [digital device] near me. (Non-null rows: 5776)\n",
      " - ST345Q01JA: Agree/disagree: I get nervous easily. (Non-null rows: 3444)\n",
      " - ST345Q03JA: Agree/disagree: I worry about many things. (Non-null rows: 3434)\n",
      " - ST345Q07JA: Agree/disagree: I feel nervous about approaching exams. (Non-null rows: 3420)\n",
      " - ST345Q10JA: Agree/disagree: I am afraid of many things. (Non-null rows: 3490)\n",
      " - ST313Q05JA: Agree/disagree: I stay calm even in tense situations. (Non-null rows: 3446)\n",
      " - ST292Q01JA: Agree/disagree: I often worry that it will be difficult for me in mathematics classes. (Non-null rows: 5390)\n",
      " - ST292Q02JA: Agree/disagree: I get very tense when I have to do mathematics homework. (Non-null rows: 5438)\n",
      " - ST292Q03JA: Agree/disagree: I get very nervous doing mathematics problems. (Non-null rows: 5357)\n",
      " - ST292Q04JA: Agree/disagree: I feel helpless when doing a mathematics problem. (Non-null rows: 5350)\n",
      " - ST292Q05JA: Agree/disagree: I worry that I will get poor [marks] in mathematics. (Non-null rows: 5334)\n",
      " - ST324Q02JA: Agree/disagree: I worry that I am not prepared for life after [the final year of compulsory education]. (Non-null rows: 3205)\n",
      " - ST324Q07JA: Agree/disagree: I worry that I won't have enough money to do what I'd like to do after [the final year of compulsory education]. (Non-null rows: 3159)\n",
      "\n",
      "Checking for 'Self-Efficacy' (Confidence) variables:\n",
      " - MATHEFF (Rows: 6584)\n"
     ]
    }
   ],
   "source": [
    "# 1. Search the Metadata for keywords\n",
    "print(\"Searching metadata for 'Anxiety', 'Worry', 'Tense', or 'Math'...\")\n",
    "\n",
    "# Get the map of Column Name -> Description\n",
    "col_labels = meta.column_names_to_labels\n",
    "\n",
    "found_vars = []\n",
    "keywords = ['worry', 'tense', 'nervous', 'helpless', 'anxiety', 'fear', 'afraid']\n",
    "\n",
    "for col_name, description in col_labels.items():\n",
    "    # Check if any keyword is in the description (case insensitive)\n",
    "    if any(k in description.lower() for k in keywords):\n",
    "        # Check if the column actually exists in our PH DataFrame\n",
    "        if col_name in ph_df.columns:\n",
    "            # Check if it's not all Empty/NaN\n",
    "            non_null_count = ph_df[col_name].notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                found_vars.append((col_name, description, non_null_count))\n",
    "\n",
    "# Print what we found\n",
    "print(f\"\\nFound {len(found_vars)} potential Anxiety/Emotion variables:\")\n",
    "for v in found_vars:\n",
    "    print(f\" - {v[0]}: {v[1]} (Non-null rows: {v[2]})\")\n",
    "\n",
    "# --- Special Check for the Best Proxy: Self-Efficacy ---\n",
    "# If Anxiety is missing, Low Self-Efficacy is the best substitute.\n",
    "print(\"\\nChecking for 'Self-Efficacy' (Confidence) variables:\")\n",
    "efficacy_cols = [c for c in ph_df.columns if 'MATHEFF' in c or 'ST188' in c]\n",
    "for c in efficacy_cols:\n",
    "     print(f\" - {c} (Rows: {ph_df[c].notna().sum()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANXMATH Feature Engineering\n",
    "\n",
    "The variables ST292Q01JA through ST292Q05JA are the actual raw questions PISA uses to calculate the ANXMAT index. The reason ANXMAT was empty in your file is likely because the aggregation wasn't pre-calculated.\n",
    "\n",
    "### We combine this with MATHEFF (Self-Efficacy)\n",
    "1. Scientific Validity: Anxiety (Fear) and Self-Efficacy (Confidence) are two sides of the same coin. High Anxiety usually means Low Confidence.\n",
    "2. Data Coverage (The \"Smart Fill\"): Due to \"Matrix Sampling,\" Student A might have answered the Anxiety questions (ST292), while Student B answered the Confidence questions (MATHEFF/ST188). By using both, we can \"triangulate\" the risk for far more students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Raw Anxiety Columns...\n",
      "Created 'ANXMAT_NEW'. Valid values: 7193\n",
      "\n",
      "Integrating MATHEFF for Smart Imputation...\n",
      "Smart Imputation Complete: Used Confidence to predict missing Anxiety.\n",
      "Final ANXMAT ready. Rows: 7193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# 1. Define the Raw Columns you found\n",
    "anxiety_cols = ['ST292Q01JA', 'ST292Q02JA', 'ST292Q03JA', 'ST292Q04JA', 'ST292Q05JA']\n",
    "\n",
    "# 2. Check and Fix Directions\n",
    "# In PISA: 1=Strongly Agree (Bad), 4=Strongly Disagree (Good)\n",
    "# We want: 4=High Anxiety (Bad), 1=Low Anxiety (Good)\n",
    "print(\"Processing Raw Anxiety Columns...\")\n",
    "for col in anxiety_cols:\n",
    "    # Fill tiny gaps with median first (for students who missed just 1 question)\n",
    "    ph_df[col] = ph_df[col].fillna(ph_df[col].median())\n",
    "    \n",
    "    # Invert the scale (Formula: New_Score = 5 - Old_Score)\n",
    "    ph_df[f'{col}_FLIPPED'] = 5 - ph_df[col]\n",
    "\n",
    "# 3. Create the \"Custom Anxiety Index\" (Average of flipped items)\n",
    "flipped_cols = [f'{c}_FLIPPED' for c in anxiety_cols]\n",
    "ph_df['ANXMAT_NEW'] = ph_df[flipped_cols].mean(axis=1)\n",
    "\n",
    "print(f\"Created 'ANXMAT_NEW'. Valid values: {ph_df['ANXMAT_NEW'].notna().sum()}\")\n",
    "\n",
    "# --- COMBINING WITH MATHEFF (Smart Imputation) ---\n",
    "\n",
    "if 'MATHEFF' in ph_df.columns:\n",
    "    print(\"\\nIntegrating MATHEFF for Smart Imputation...\")\n",
    "    \n",
    "    # Define columns to use for the \"Pattern Filling\"\n",
    "    impute_cols = ['ANXMAT_NEW', 'MATHEFF']\n",
    "    \n",
    "    # Initialize the Smart Imputer\n",
    "    imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "    \n",
    "    # Create a mask for rows that have at least one value (don't impute totally empty rows)\n",
    "    mask = ph_df[impute_cols].notna().any(axis=1)\n",
    "    \n",
    "    # This magically fills missing Anxiety using the pattern found in Efficacy!\n",
    "    ph_df.loc[mask, impute_cols] = imputer.fit_transform(ph_df.loc[mask, impute_cols])\n",
    "    \n",
    "    print(\"Smart Imputation Complete: Used Confidence to predict missing Anxiety.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nMATHEFF column not found directly. Using simple median fill.\")\n",
    "    ph_df['ANXMAT_NEW'] = ph_df['ANXMAT_NEW'].fillna(ph_df['ANXMAT_NEW'].median())\n",
    "\n",
    "# 4. Finalize\n",
    "ph_df['ANXMAT'] = ph_df['ANXMAT_NEW']\n",
    "print(f\"Final ANXMAT ready. Rows: {len(ph_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph_df['Gender'] = ph_df['ST004D01T'].map({1.0: 'Female', 2.0: 'Male'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Pickle (Fastest, preserves all Python data types)\n",
    "ph_df.to_pickle(\"ph_pisa_2022_filtered.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit: Check for Official PISA indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AUDIT: Checking for Official PISA Indices ---\n",
      "✅ FOUND: ANXMAT (Math Anxiety (Custom/Fixed)) - 7193 rows\n",
      "✅ FOUND: BELONG (Sense of Belonging) - 6973 rows\n",
      "✅ FOUND: ESCS (Socio-Economic Status) - 7167 rows\n",
      "✅ FOUND: TEACHSUP (Teacher Support) - 6755 rows\n",
      "✅ FOUND: BULLIED (Exposure to Bullying) - 6990 rows\n",
      "❌ MISSING: GROWMIND (Growth Mindset)\n",
      "✅ FOUND: ICTRES (ICT Resources) - 7152 rows\n",
      "❌ MISSING: PERCOMP (Perceived Competence (Alternative to Mindset?))\n",
      "\n",
      "Final Feature Set for Model: ['ANXMAT', 'BELONG', 'ESCS', 'TEACHSUP', 'BULLIED', 'ICTRES']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. List of Official PISA 2022 Derived Indices we hope to find\n",
    "# (We already fixed ANXMAT, so we use our custom one for that)\n",
    "potential_features = {\n",
    "    'ANXMAT': 'Math Anxiety (Custom/Fixed)', \n",
    "    'BELONG': 'Sense of Belonging', \n",
    "    'ESCS': 'Socio-Economic Status', \n",
    "    'TEACHSUP': 'Teacher Support', \n",
    "    'BULLIED': 'Exposure to Bullying',\n",
    "    'GROWMIND': 'Growth Mindset',\n",
    "    'ICTRES': 'ICT Resources',\n",
    "    'PERCOMP': 'Perceived Competence (Alternative to Mindset?)'\n",
    "}\n",
    "\n",
    "print(\"--- AUDIT: Checking for Official PISA Indices ---\")\n",
    "valid_features = []\n",
    "\n",
    "for col, desc in potential_features.items():\n",
    "    if col in ph_df.columns:\n",
    "        # Check if it has data (not just empty column)\n",
    "        count = ph_df[col].notna().sum()\n",
    "        if count > 0:\n",
    "            print(f\"✅ FOUND: {col} ({desc}) - {count} rows\")\n",
    "            valid_features.append(col)\n",
    "        else:\n",
    "            print(f\"❌ EMPTY: {col} exists but is null.\")\n",
    "    else:\n",
    "        print(f\"❌ MISSING: {col} ({desc})\")\n",
    "\n",
    "# 3. Update the features list dynamically\n",
    "print(f\"\\nFinal Feature Set for Model: {valid_features}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
